{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oscar Meurer\n",
    "\n",
    "Guillaume Ferreol\n",
    "\n",
    "Nicolas Reboullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVES:\n",
    "\n",
    "Write a Python notebook that will \n",
    "1) explain with your own words what information the paper brings with respect to what we studied in class and \n",
    "2) illustrate the results of the paper by running some experiments. \n",
    "\n",
    "You should design these experiments in order to highlight the pros and the cons of the method proposed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale-Free Algorithms for Online Linear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1502.05744v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper 4: this paper studies a a general family of algorithms known as FTRL (Follow The Regularized Leader), but note that when the regularizer is R(w) = ||w||^2, this is exactly the same as OGA. \n",
    "- In class, we calibrated the gradient step in OGA as depending on the Lipschitz constant of the functions to optimize. \n",
    "- This paper proposes two methods that does not require to know this constant: AdaFTRL and Solo FTRL. \n",
    "- Focus on the second one: Solo FTRL. Describe the method, explain how Theorem 2 improves on the results seen in class. Illustrate this improvement on simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical reminder of Online Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Online Gradient Algorithm (OGA) can be described as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\text{Initialize: } \\theta_0 \\\\\n",
    "& \\text{For } t = 1, 2, \\ldots \\\\\n",
    "& \\quad \\text{Receive } x_t \\\\\n",
    "& \\quad \\text{Compute loss } \\ell_t = \\ell(f(x_t; \\theta_{t-1}), y_t) \\\\\n",
    "& \\quad \\text{Compute gradient } g_t = \\nabla_{\\theta_{t-1}} \\ell_t \\\\\n",
    "& \\quad \\text{Update: } \\theta_t = \\theta_{t-1} - \\eta g_t\n",
    "\\end{align*}\n",
    "\n",
    "where $\\theta_t$ represents the model parameters at time $t$, $x_t$ is the input data at time $t$, $f$ is the model function, $y_t$ is the true label or target at time $t$, $\\ell$ is the loss function, $g_t$ is the gradient of the loss function with respect to the model parameters, and $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem on the regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Title](theorem_OGA_regret.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of what the paper brings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
