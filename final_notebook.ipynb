{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oscar Meurer\n",
    "\n",
    "Guillaume Ferreol\n",
    "\n",
    "Nicolas Reboullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBJECTIVES:\n",
    "\n",
    "Write a Python notebook that will \n",
    "1) explain with your own words what information the paper brings with respect to what we studied in class and \n",
    "2) illustrate the results of the paper by running some experiments. \n",
    "\n",
    "You should design these experiments in order to highlight the pros and the cons of the method proposed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale-Free Algorithms for Online Linear Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1502.05744v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Paper 4: this paper studies a a general family of algorithms known as FTRL (Follow The Regularized Leader), but note that when the regularizer is R(w) = ||w||^2, this is exactly the same as OGA. \n",
    "- In class, we calibrated the gradient step in OGA as depending on the Lipschitz constant of the functions to optimize. \n",
    "- This paper proposes two methods that does not require to know this constant: AdaFTRL and Solo FTRL. \n",
    "- Focus on the second one: Solo FTRL. Describe the method, explain how Theorem 2 improves on the results seen in class. Illustrate this improvement on simulations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
